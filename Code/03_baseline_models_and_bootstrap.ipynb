{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b1de40-12e9-4854-bd77-d793038af035",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457b145-7b9d-4fd1-b2b1-d0eb80ecc861",
   "metadata": {},
   "source": [
    "This code splits the labelled data into train, validation, and test sets, and generates predictions using 3 baseline models: a rule-based dictionary, Naive Bayes, and SVM (BoW).\n",
    "\n",
    "Hyperparameters are set using a grid search, and the best models are further statistically compared by re-running training and evaluation on different random splits of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a14b8-cf29-445b-9179-4c998f338a9a",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f2d97-44a0-4890-85ee-f7b4e00dbe8d",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1504012-e895-442b-86b3-566d50f4fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from textwrap import wrap\n",
    "\n",
    "# sklearn\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb0eca-ba44-4b8a-97b5-9a6a22c6ec91",
   "metadata": {},
   "source": [
    "### Import train, val, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb3b96-fa6c-4232-9357-e17d65f07d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../Data/train.pkl')\n",
    "val = pd.read_pickle('../Data/val.pkl')\n",
    "test = pd.read_pickle('../Data/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c98e8-5933-4448-b664-a824d340294b",
   "metadata": {},
   "source": [
    "### Functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e6deb-2285-49d7-878c-c6e6cdf4e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11fd087-7775-4967-8b38-c79810ac4246",
   "metadata": {},
   "source": [
    "### Check dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c35874-2105-4df1-a3f4-a37de3632910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in zip(['Train','Val','Test'],[train, val, test]):\n",
    "    print(f\"{name}: {len(dataset)} paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1302c-a21b-46bd-819b-fe924c2bcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN:\")\n",
    "for col in ['Process_action','Market_action','Environment','Social']:\n",
    "    print(f\"{col}: {len(train[train[col]==1])*100/len(train):.2f}% ({len(train[train[col]==1])}) are true\")\n",
    "    \n",
    "print(\"\\nVAL:\")\n",
    "for col in ['Process_action','Market_action','Environment','Social']:\n",
    "    print(f\"{col}: {len(val[val[col]==1])*100/len(val):.2f}% ({len(val[val[col]==1])}) are true\")\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "for col in ['Process_action','Market_action','Environment','Social']:\n",
    "    print(f\"{col}: {len(test[test[col]==1])*100/len(test):.2f}% ({len(test[test[col]==1])}) are true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2d92b-c925-4279-b8e4-f1bc2b45a921",
   "metadata": {},
   "source": [
    "### Import augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e2d87-fdb9-4f1e-8d47-1a1321440a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "translated_data = pd.read_excel('../Data/Back translation with GT.xlsx')\n",
    "print(translated_data.dtypes)\n",
    "translated_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab1e1d-b90a-409c-bdf8-d626a9705cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set correct dtypes\n",
    "data_for_augmentation = translated_data[translated_data['Exact match']=='No']\n",
    "data_for_augmentation.rename(columns={'Back-translation':'Paragraph'}, inplace=True)\n",
    "data_for_augmentation.drop(columns=['Original Paragraph','Exact match'], inplace=True)\n",
    "data_for_augmentation.set_index('Index',inplace=True)\n",
    "data_for_augmentation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425ab92-7240-48f1-8566-063da12fa595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_dataset(df):\n",
    "    translated_indices = data_for_augmentation.index.to_list() #translated indices\n",
    "    market_indices = df.index.to_list() #indices in original df\n",
    "    indices = list(set(translated_indices) & set(market_indices)) # only check indices in both dfs\n",
    "    translated_set = data_for_augmentation.loc[indices]\n",
    "    augmented_dataset = shuffle(pd.concat([df, translated_set]), random_state=42)\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd66c8-de47-4062-9225-e704ea8ce09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = get_augmented_dataset(train)\n",
    "augmented_val = get_augmented_dataset(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b62991-b851-40df-ba4f-14fbb6470db3",
   "metadata": {},
   "source": [
    "## Model fitting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ee381-9838-4e9a-ae3a-12aad24fda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_has_keyword(text, unigrams, bigrams):\n",
    "    text = text.strip()\n",
    "    text = re.sub('[,\\.!?]', '', text) # remove punctuation\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # removes non-letter characters\n",
    "    words = text.lower().split(' ') # make lowercase & split\n",
    "    words = [word for word in words if word.strip()]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text_in_bigrams = [(ele, words[i+1]) for i, ele in enumerate(words) if i < len(words)-1]\n",
    "    # check unigrams\n",
    "    if any(word==unigram for word in words for unigram in unigrams):\n",
    "        return 1\n",
    "    elif any(tb==bigram for tb in text_in_bigrams for bigram in bigrams):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def fit_cv(param_grid, X, y):\n",
    "    '''\n",
    "    Gets best params from given param_grid\n",
    "    '''\n",
    "    #Tune hyperparameters\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('clf-svm', SVC(random_state=42))])\n",
    "    grid = GridSearchCV(model, param_grid, scoring='f1_macro')\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    #Return best parameters\n",
    "    return grid.best_params_, grid.best_score_\n",
    "\n",
    "def svm_grid_search(x_val, y_val, param_grids):\n",
    "    best_params_all = []\n",
    "    for param_grid in param_grids:\n",
    "        best_params, best_score = fit_cv(param_grid, x_val, y_val)\n",
    "        best_params_all.append((best_params, best_score))\n",
    "    best_params_final, best_score_final = max(best_params_all, key = lambda i : i[1])\n",
    "    print(f\"Best score: {best_score_final}\")\n",
    "    print(f\"Best params: {best_params_final}\")\n",
    "    return best_params_final\n",
    "\n",
    "def get_best_parameters(pipeline, col, parameters, data_val):\n",
    "    gs_model = GridSearchCV(pipeline, parameters, scoring='f1_macro')\n",
    "    gs_model.fit(data_val['Paragraph'], data_val[col])\n",
    "    print(f\"Best score: {gs_model.best_score_}\")\n",
    "    print(f\"Best params: {gs_model.best_params_}\")\n",
    "    return gs_model.best_params_\n",
    "\n",
    "def run_best_parameters(model, best_params, col, data_train):\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(data_train['Paragraph'], data_train[col])\n",
    "    preds = model.predict(test['Paragraph'])\n",
    "    print_classification_report_heatmap(test[col], preds)\n",
    "    return model, preds\n",
    "\n",
    "def print_classification_report_heatmap(actuals, preds):\n",
    "    # Get classification report\n",
    "    print(classification_report(actuals, preds, digits = 4))\n",
    "    \n",
    "    # Get AUROC score\n",
    "    print(f\"ROC AUC score: {roc_auc_score(actuals, preds)}\") #returns macro by default\n",
    "\n",
    "    # Get heatmap\n",
    "    fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    process_mat = confusion_matrix(actuals, preds)\n",
    "    sns.heatmap(process_mat.T, square = True, annot=True, fmt = \"d\", ax=ax)\n",
    "    ax.set_xlabel(\"true labels\")\n",
    "    ax.set_ylabel(\"predicted labels\")\n",
    "    plt.show()\n",
    "    \n",
    "def find_and_run_best_model(pipeline, data_train, data_val, col, parameters, svm=False):\n",
    "    if svm:\n",
    "        best_params = svm_grid_search(data_val['Paragraph'], data_val[col], parameters)\n",
    "    else:\n",
    "        best_params = get_best_parameters(pipeline, col, parameters, data_val)\n",
    "    model, preds = run_best_parameters(pipeline, best_params, col, data_train)\n",
    "    return model, preds, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc22e5a-73a1-4e9d-9a9e-30a7ade9ecda",
   "metadata": {},
   "source": [
    "## Find and run best baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eda69e-088f-45ca-9d40-482fc903b668",
   "metadata": {},
   "source": [
    "### Keyword based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b30e1d-e409-4cf3-adf1-c4dc7952db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All keyword sets\n",
    "process_keywords = ['policy', 'audit','visit','questionnaire','certification','policies',\n",
    "                    'requirement','assessment','certify','certifies',\n",
    "                    'certified','compliance','complies','complied','comply',\n",
    "                    'noncompliance','approved','approval','termination','terminate','terminated',\n",
    "                    'terminates','corrective','code',\n",
    "                    'award','screening','recycled','fsc','certified',\n",
    "                    'preferred','monitor','zdhc','remediation','remediate','remediated','remediates']\n",
    "\n",
    "process_bigrams = [('preferred','supplier'), ('supplier','diversity'), ('local','supplier')]\n",
    "\n",
    "market_keywords = ['reformulate','reformulated','reformulates','reformulation',\n",
    "                   'circular','restructuring','platform','disclose','disclosed','discloses',\n",
    "                   'coalition','partnership','redesign','publish','published','publishes',\n",
    "                   'affliated','member','partner']\n",
    "\n",
    "market_bigrams = [('closed','loop')]\n",
    "\n",
    "social_keywords = ['health', 'safety','labor','labour', 'local', 'quality', \n",
    "                   'corruption', 'human', 'occupational', 'conflict', 'collective', \n",
    "                   'discrimination', 'community', 'donation', 'donate', 'donates', \n",
    "                   'donated', 'antibiotic', 'rana', 'uzbekistan', 'drc', 'zimbabwe', \n",
    "                   '2600', 'rmap', 'sedex', '16949','accident']\n",
    "\n",
    "social_bigrams = [('better','cotton'), ('responsible','mineral'), ('ethical','trade'), \n",
    "                  ('collective','bargaining'), ('lost','day'), ('data','confidentiality')]\n",
    "\n",
    "env_keywords = ['waste','energy','emission','biodiversity','gmo','genetic','ipm','chemical',\n",
    "                'bepi', 'imds', '14001', 'msc', 'eutr', 'eco', 'carbon', 'co', 'co2', \n",
    "                'environmental', 'water', 'pollution', 'plastic', 'packaging']\n",
    "\n",
    "env_bigrams = [('better','cotton')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5298bf-645f-4d45-8db6-3ba14df616dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "process_predicted_kw = test['Paragraph'].map(lambda x: check_if_has_keyword(x, process_keywords, process_bigrams))\n",
    "print_classification_report_heatmap(test['Process_action'], process_predicted_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefede1e-c2fe-487c-b1f5-250bd22277ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market\n",
    "market_predicted_kw = test['Paragraph'].map(lambda x: check_if_has_keyword(x, market_keywords, market_bigrams))\n",
    "print_classification_report_heatmap(test['Market_action'], market_predicted_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2bd0a-0aaa-45fb-8665-ceb8f9a9c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social\n",
    "social_predicted_kw = test['Paragraph'].map(lambda x: check_if_has_keyword(x, social_keywords, social_bigrams))\n",
    "print_classification_report_heatmap(test['Social'], social_predicted_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21466eb3-d6d2-4f5e-ae13-1f8f586178bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env_predicted_kw = test['Paragraph'].map(lambda x: check_if_has_keyword(x, env_keywords, env_bigrams))\n",
    "print_classification_report_heatmap(test['Environment'], env_predicted_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dcc5a-8296-41a3-af7d-f7562d0afc98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d47999-286b-46c2-8448-4d692677ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'nb__norm': (True, False),\n",
    "                 'nb__alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]}\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('nb', ComplementNB())])\n",
    "\n",
    "nb_dict = dict()\n",
    "for col in ['Process_action','Market_action','Social','Environment']:\n",
    "    print(f\"Classification task: {col}\")\n",
    "    model, preds, best_params = find_and_run_best_model(nb_pipeline, train, val, col, nb_parameters, svm=False)\n",
    "    nb_dict[col] = {'Model': model, 'Preds': preds, 'Best_params': best_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e91983-c46f-46fb-a64a-3d0bd25b44f6",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcdf12-d610-4287-b988-96297a559a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__C': [0.1, 1, 10, 100],\n",
    "                  'clf-svm__kernel': ['linear']}\n",
    "\n",
    "rbf_param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__C': [0.1, 1, 10, 100],\n",
    "                  'clf-svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                  'clf-svm__kernel': ['rbf']}\n",
    "\n",
    "poly_param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                   'tfidf__use_idf': (True, False),\n",
    "                   'clf-svm__C': [0.1, 1, 10, 100],\n",
    "                   'clf-svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                   'clf-svm__kernel': ['poly'], \n",
    "                   'clf-svm__degree': [2,3,4]}\n",
    "svm_parameters = [lin_param_grid, rbf_param_grid, poly_param_grid]\n",
    "\n",
    "svm_dict = dict()\n",
    "for col in ['Process_action','Market_action','Social','Environment']:\n",
    "    print(f\"Classification task: {col}\")\n",
    "    svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('clf-svm', SVC(random_state=42))])\n",
    "    model, preds, best_params = find_and_run_best_model(svm_pipeline, train, val, col, svm_parameters, svm=True)\n",
    "    svm_dict[col] = {'Model': model, 'Preds': preds, 'Best_params': best_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf2562-abd5-4dbd-955e-84ce6a88c15c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Augmented NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97736e-6225-4bb7-81fb-f10230ef41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'nb__norm': (True, False),\n",
    "                 'nb__alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]}\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('nb', ComplementNB())])\n",
    "\n",
    "aug_nb_dict = dict()\n",
    "for col in ['Process_action','Market_action','Social','Environment']:\n",
    "    model, preds, best_params = find_and_run_best_model(nb_pipeline, augmented_train, \n",
    "                                                        augmented_val, col, nb_parameters, svm=False)\n",
    "    aug_nb_dict[col] = {'Model': model, 'Preds': preds, 'Best_params': best_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43f42c-6e91-4f73-8b25-0b1bfb3886df",
   "metadata": {},
   "source": [
    "### Augmented SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab823eb-c8f6-4341-a17b-3e1459d33b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__C': [0.1, 1, 10, 100],\n",
    "                  'clf-svm__kernel': ['linear']}\n",
    "\n",
    "rbf_param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__C': [0.1, 1, 10, 100],\n",
    "                  'clf-svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                  'clf-svm__kernel': ['rbf']}\n",
    "\n",
    "poly_param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                   'tfidf__use_idf': (True, False),\n",
    "                   'clf-svm__C': [0.1, 1, 10, 100],\n",
    "                   'clf-svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                   'clf-svm__kernel': ['poly'], \n",
    "                   'clf-svm__degree': [2,3,4]}\n",
    "svm_parameters = [lin_param_grid, rbf_param_grid, poly_param_grid]\n",
    "\n",
    "aug_svm_dict = dict()\n",
    "\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                     ('clf-svm', SVC(random_state=42))])\n",
    "for col in ['Process_action','Market_action','Social','Environment']:\n",
    "    model, preds, best_params = find_and_run_best_model(svm_pipeline, augmented_train, \n",
    "                                                        augmented_val, col, svm_parameters, svm=True)\n",
    "    aug_svm_dict[col] = {'Model': model, 'Preds': preds, 'Best_params': best_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6bab4-6359-41ae-93e2-50882054dad5",
   "metadata": {},
   "source": [
    "## Save dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d4acc-f745-4ac5-b3c9-a4c5b41c855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_models = {'nb_dict': nb_dict, 'svm_dict': svm_dict,\n",
    "                   'aug_nb_dict': aug_nb_dict, 'aug_svm_dict': aug_svm_dict}\n",
    "\n",
    "with open(\"../Data/baseline_models.txt\", \"wb\") as f:\n",
    "    pickle.dump(baseline_models, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7c09c-c4bb-4081-b63c-7fc17c74423c",
   "metadata": {},
   "source": [
    "## Statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8e9a4-05bb-474e-bd97-bbfa82c7ad14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model_n30(model, params, data, col, keywords=None, bigrams=None, augmented=False):\n",
    "    f1_scores = []\n",
    "    for i in range(0,30):\n",
    "        train_stat, test_stat = train_test_split(data, \n",
    "                                   test_size=0.2, \n",
    "                                   random_state=i, \n",
    "                                   stratify=data['All4'])\n",
    "        if keywords:\n",
    "            preds = test_stat['Paragraph'].map(lambda x: check_if_has_keyword(x, keywords, bigrams))\n",
    "        else:\n",
    "            if augmented == True:\n",
    "                train_stat = get_augmented_dataset(train_stat) # augment train with back-translations\n",
    "            model.set_params(**params)\n",
    "            model.fit(train_stat['Paragraph'], train_stat[col])\n",
    "            preds = model.predict(test_stat['Paragraph'])\n",
    "        f1_scores.append(f1_score(test_stat[col], preds, average='macro'))\n",
    "    \n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4c753-7e61-4999-804f-36ae04749a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dict of dicts to store results\n",
    "model_stat_test = {'Process_action':{}, \n",
    "                   'Market_action':{},\n",
    "                   'Social':{},\n",
    "                   'Environment':{}}\n",
    "\n",
    "# Run models 30 times on different random splits of train data (using best hyperparameters)\n",
    "\n",
    "# Keyword\n",
    "for col, keywords, bigrams in zip(['Process_action', 'Market_action', \n",
    "                          'Social', 'Environment'], \n",
    "                         [process_keywords, market_keywords, \n",
    "                          social_keywords, env_keywords], \n",
    "                         [process_bigrams, market_bigrams, \n",
    "                          social_bigrams, env_bigrams]):\n",
    "    f1_scores = run_model_n30(model, None, train, col, keywords=keywords, bigrams=bigrams)\n",
    "    model_stat_test[col]['Dictionary'] = f1_scores\n",
    "\n",
    "# Naive Bayes\n",
    "for col, inner_dict in nb_dict.items():\n",
    "    best_params = inner_dict['Best_params']\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('nb', ComplementNB())])\n",
    "    f1_scores = run_model_n30(model, best_params, train, col)\n",
    "    model_stat_test[col]['Naive Bayes'] = f1_scores\n",
    "\n",
    "# SVM\n",
    "for col, inner_dict in svm_dict.items():\n",
    "    best_params = inner_dict['Best_params']\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('clf-svm', SVC(random_state=42))])\n",
    "    f1_scores = run_model_n30(model, best_params, train, col)\n",
    "    model_stat_test[col]['SVM'] = f1_scores\n",
    "    \n",
    "# Augmented Naive Bayes\n",
    "nb_model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('nb', ComplementNB())])\n",
    "for col, inner_dict in aug_nb_dict.items():\n",
    "    best_params = inner_dict['Best_params']\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('nb', ComplementNB())])\n",
    "    f1_scores = run_model_n30(model, best_params, train, col, augmented=True)\n",
    "    model_stat_test[col]['Augmented Naive Bayes'] = f1_scores\n",
    "\n",
    "# Augmented SVM\n",
    "svm_model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('clf-svm', SVC(random_state=42))])\n",
    "for col, inner_dict in aug_svm_dict.items():\n",
    "    best_params = inner_dict['Best_params']\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                         ('clf-svm', SVC(random_state=42))])\n",
    "    f1_scores = run_model_n30(model, best_params, train, col, augmented=True)\n",
    "    model_stat_test[col]['Augmented SVM'] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498278c6-891c-4fcb-82d1-34e1c88d13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dict into DataFrame\n",
    "model_stat_test_df = pd.DataFrame(model_stat_test)\n",
    "model_stat_test_df = model_stat_test_df.reset_index().rename(columns={'index':'Approach'})\n",
    "model_stat_test_df = model_stat_test_df.melt(id_vars=['Approach'], var_name='Module', value_name='F1').dropna()\n",
    "model_stat_test_df = model_stat_test_df.explode('F1').reset_index(drop=True)\n",
    "model_stat_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845ab80-f2ca-489b-a840-03e984507064",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat_test_df.to_pickle('../Data/baseline_model_stat_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
