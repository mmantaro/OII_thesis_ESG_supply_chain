{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d83222-fed9-4995-9cdf-b2ec5143f371",
   "metadata": {
    "id": "69d83222-fed9-4995-9cdf-b2ec5143f371"
   },
   "source": [
    "# ESG report text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc4eb5-eb8e-4eb5-9bc6-6dd12601b217",
   "metadata": {
    "id": "48dc4eb5-eb8e-4eb5-9bc6-6dd12601b217"
   },
   "source": [
    "This code retrieves converts PDFs (from both online links and downloaded files) into plain text using the PyMuPDF library. The extracted text is saved locally for each report as a .txt file. \n",
    "\n",
    "Two dataframes are created for future analysis:\n",
    "- **Report details**: Saves report-level details (e.g. number of pages, number of paragraphs, number of supply chain paragraphs)\n",
    "- **Paragraph**: Dataframe where each row is a paragraph; includes meta-data (filename, year, company ID), the paragraph text, and whether the paragraph is supply-chain-related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434488a-0243-4505-a6eb-aeceb89b72a0",
   "metadata": {
    "id": "0434488a-0243-4505-a6eb-aeceb89b72a0"
   },
   "source": [
    "## Import & set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd14f84-a5fa-4cde-8309-42f2e791046c",
   "metadata": {
    "id": "8dd14f84-a5fa-4cde-8309-42f2e791046c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import os.path\n",
    "import io\n",
    "import fitz # install using: pip install PyMuPDF\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import sqlite3\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from re import search\n",
    "import urllib\n",
    "import pickle\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b5c3a-b7ef-4997-a395-0741ce50b163",
   "metadata": {
    "id": "c86b5c3a-b7ef-4997-a395-0741ce50b163"
   },
   "source": [
    "## Upload list of links and file locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fad3d3-afa3-448d-9733-86e322b9c6f1",
   "metadata": {},
   "source": [
    "The links to the PDFs were manually collected in an Excel file where each row is a company and there are columns for each year. There could be multiple reports in a given year. When this was the case, I included all links separated by ' and '. For a few companies, I manually downloaded the PDFs while collecting the link (either because they automatically downloaded or because they weren't stored at a .pdf link, for example if the company used ISSUU to host the files). In these cases, the links would refer to filenames within a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2fe8e-7952-4e76-b0cf-2c9758794420",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "8dd2fe8e-7952-4e76-b0cf-2c9758794420",
    "outputId": "5fe48851-e339-46f1-ba54-678be478b4c1"
   },
   "outputs": [],
   "source": [
    "# Upload file\n",
    "esg_urls_pd = pd.read_excel('../Data/report_links.xlsx')\n",
    "esg_urls_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91bc66-2d90-4145-9aa5-051fb52a32b3",
   "metadata": {
    "id": "7f91bc66-2d90-4145-9aa5-051fb52a32b3"
   },
   "outputs": [],
   "source": [
    "# Clean dataframe\n",
    "esg_urls_pd.rename(columns={'Exchange:Ticker':'Exchange-Ticker'},inplace=True)\n",
    "esg_urls_pd['Exchange-Ticker'] = esg_urls_pd['Exchange-Ticker'].str.replace(':','-')\n",
    "\n",
    "# Change from wide to long format\n",
    "esg_urls_pd_m = esg_urls_pd.melt(id_vars=['Company Name','Exchange-Ticker','Primary Sector'], \n",
    "                 value_vars=[2012,2013,2014,2015,2016,\n",
    "                             2017,2018,2019,2020,2021],value_name='URL',var_name='Year')\n",
    "esg_urls_pd_m['URL'] = esg_urls_pd_m['URL'].str.split(' and ')\n",
    "esg_urls_pd_m = esg_urls_pd_m.explode('URL')\n",
    "esg_urls_pd_m.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f1b94-6227-42ff-87d8-51fb0fb6effe",
   "metadata": {
    "id": "f65f1b94-6227-42ff-87d8-51fb0fb6effe"
   },
   "outputs": [],
   "source": [
    "# Only need to download reports with URL starting with \"http\"\n",
    "esg_urls_pd_m['to_download'] = esg_urls_pd_m['URL'].apply(lambda x: False if (pd.isna(x) or x[:4]!='http') else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f8f93-5ec1-4ee0-9b29-00d92da75c53",
   "metadata": {
    "id": "078f8f93-5ec1-4ee0-9b29-00d92da75c53"
   },
   "outputs": [],
   "source": [
    "# Set filenames such that reports from the same year have unique filenames\n",
    "esg_urls_pd_m['dup_num'] = esg_urls_pd_m.groupby(['Exchange-Ticker','Year']).cumcount()+1\n",
    "esg_urls_pd_m['filename'] = esg_urls_pd_m.apply(lambda x: f\"{x['Exchange-Ticker']}-{x['Year']}-{x['dup_num']}\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CKAQGWH1essM",
   "metadata": {
    "id": "CKAQGWH1essM"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28cfe2-8369-47e6-a75b-29ea6b3bc1b1",
   "metadata": {},
   "source": [
    "This section defines a number of functions that are needed to process the PDFs and clean & save text:\n",
    "- **Set-up**: Initializes key variables and the SpaCy model\n",
    "- **Ordering functions**: Defines custom functions that determine the ordering of text blocks when extracting text with PyMuPDF; this is necesssary because many ESG reports have pages with multiple columns or even multiple blocks of multi-column text.\n",
    "- **Text cleaning & other helper functions**: Defines a variety of custom helper functions that are used when extracting & cleaning text\n",
    "- **Main text extraction functions**: Defines functions that are called when extracting text from a PDF link (or local file location). \n",
    "- **Text to dict function**: Transforms a plain text file into a dict (which are later used to create a dataframe) that includes both paragraph text and whether the paragraph is supply chain related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51xvbgEt1KSH",
   "metadata": {
    "id": "51xvbgEt1KSH"
   },
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UBZiS1Sva3rN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBZiS1Sva3rN",
    "outputId": "06f5606b-bf65-4fcb-b25e-bbad1544ea4d"
   },
   "outputs": [],
   "source": [
    "# Enable checking for non ASCII characters\n",
    "printable = set(string.printable)\n",
    "\n",
    "# Load spacy model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "# Get list of English words\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\") as word_file:\n",
    "    english_words = set(word.decode(\"utf-8\").strip().lower() for word in word_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a-wveUpz1N1j",
   "metadata": {
    "id": "a-wveUpz1N1j"
   },
   "source": [
    "### Ordering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VzvY5Rzq1OLx",
   "metadata": {
    "id": "VzvY5Rzq1OLx"
   },
   "outputs": [],
   "source": [
    "def custom_sort(blocks, merge_margin=20):\n",
    "    filtered_blocks = [b for b in blocks if b['type'] == 0] #filter to only text blocks\n",
    "    big_boxes = group_blocks_y(filtered_blocks, merge_margin=merge_margin) # get min & max of y groups\n",
    "    sorted_blocks = []\n",
    "    # sort big boxes first by x0, then by y0\n",
    "    for box in sorted(big_boxes, \n",
    "                      key=lambda box: box[0]):\n",
    "        y0, y1 = box\n",
    "        blocks_in_box = [b for b in filtered_blocks if (b['bbox'][1] >= y0 and \n",
    "                                                        b['bbox'][1] <= y1)]\n",
    "        \n",
    "  \n",
    "        # sort blocks within box first by x0, then by y0\n",
    "        sorted_blocks.extend(sorted(blocks_in_box, \n",
    "                                    key=lambda b: (round_to_base(b['bbox'][0], 100), b['bbox'][1])))\n",
    "    #sort big blocks\n",
    "    #sort blocks within big blocks\n",
    "    return sorted_blocks\n",
    "\n",
    "def y_overlap(source, target):\n",
    "    # unpack points both boxes\n",
    "    top1, bottom1 = source\n",
    "    top2, bottom2 = target\n",
    "\n",
    "    # boxes don't overlap if y0 of one box is greater than y1 of the other box\n",
    "    if (top1 > bottom2 or top2 > bottom1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_all_y_overlaps(boxes, bounds, index):\n",
    "    overlaps = []\n",
    "    for a in range(len(boxes)):\n",
    "        if a != index:\n",
    "            if y_overlap(bounds, boxes[a]):\n",
    "                overlaps.append(a)\n",
    "    return overlaps\n",
    "\n",
    "def group_blocks_y(filtered_blocks, merge_margin=20):\n",
    "    boxes = [] # saved as list of [y0,y1]\n",
    "    for b in filtered_blocks:\n",
    "        x0, y0, x1, y1 = b['bbox'] # PyMuPDF bbox ordering\n",
    "        boxes.append([y0,y1])\n",
    "    \n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        # set end con\n",
    "        finished = True\n",
    "\n",
    "        index = len(boxes) - 1\n",
    "        while index >= 0:\n",
    "            curr = boxes[index]\n",
    "\n",
    "            # add margin\n",
    "            top = curr[0]\n",
    "            bottom = curr[1]\n",
    "            top -= merge_margin #extend upwards\n",
    "            bottom += merge_margin #extend downwards\n",
    "\n",
    "            # get matching boxes\n",
    "            overlaps = get_all_y_overlaps(boxes, [top, bottom], index)\n",
    "\n",
    "            # check if empty\n",
    "            if len(overlaps) > 0:\n",
    "                overlap_y0s = []\n",
    "                overlap_y1s = []\n",
    "\n",
    "                # combine y-coords\n",
    "                overlaps.append(index)\n",
    "\n",
    "                for ind in overlaps:\n",
    "                    top, bottom = boxes[ind]\n",
    "                    overlap_y0s.append(top)\n",
    "                    overlap_y1s.append(bottom)\n",
    "\n",
    "                merged = [min(overlap_y0s), max(overlap_y1s)]\n",
    "\n",
    "                overlaps.sort(reverse = True)\n",
    "                for ind in overlaps:\n",
    "                    del boxes[ind]\n",
    "                boxes.append(merged)\n",
    "\n",
    "                # set flag\n",
    "                finished = False;\n",
    "                break\n",
    "\n",
    "            index -= 1\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8FwqEEEn1OoE",
   "metadata": {
    "id": "8FwqEEEn1OoE"
   },
   "source": [
    "### Text cleaning & other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HhFggze11XrD",
   "metadata": {
    "id": "HhFggze11XrD"
   },
   "outputs": [],
   "source": [
    "# Checks whether a word is a valid English word\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_words\n",
    "\n",
    "# Checks if two strings are actually a single word that was incorrectly split\n",
    "def is_split_word(first, second):\n",
    "    if not (is_english_word(first.lower())) or not (is_english_word(second.lower())):\n",
    "        combined = first + second\n",
    "        return is_english_word(combined)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Because sometimes fontsizes differ by minuscule amounts,\n",
    "# need function to round\n",
    "def round_to_base(x, base=100):\n",
    "    return base * round(x/base)\n",
    "\n",
    "# Check for most common fontsize\n",
    "def get_frequency_font_sizes(doc):\n",
    "    styles = {}\n",
    "    font_counts = {}\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # block contains text\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        rounded_fontsize = round(s['size'])\n",
    "                        identifier = \"{0}\".format(rounded_fontsize)\n",
    "                        styles[identifier] = {'size': rounded_fontsize}\n",
    "                        \n",
    "                        # Count the fonts usage (by character rather than span \n",
    "                        # because some reports have many figs w/many spans)\n",
    "                        font_counts[identifier] = font_counts.get(identifier, 0) + len(s['text'].strip())\n",
    "\n",
    "    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if len(font_counts) < 1:\n",
    "        return None # No discriminating fonts were found\n",
    "    p_style = styles[font_counts[0][0]]\n",
    "    p_size = p_style['size']\n",
    "    return p_size\n",
    "\n",
    "# Check whether a space needs to be added when combining 2 lines\n",
    "def need_to_add_space(prev_line, next_line):\n",
    "    if prev_line != \"\" and not prev_line.strip().endswith('-') and not prev_line.endswith(' '): \n",
    "        # check if there seems to be a word split across lines, \n",
    "        # only if first character of new line is a lowercase letter\n",
    "        # and last character of last line is a lowercase letter\n",
    "        if next_line.strip()[0].isalpha() and next_line.strip()[0].islower() \\\n",
    "        and prev_line.strip()[-1].isalpha() and \\\n",
    "        prev_line.strip()[-1].strip()[0].islower():\n",
    "            last_word_prev = prev_line.strip().split(' ')[-1]\n",
    "            first_word_line = next_line.strip().split(' ')[0]\n",
    "            if is_split_word(last_word_prev, first_word_line):\n",
    "                return False #don't add space if split word\n",
    "            else:\n",
    "                return True #add space if not split word\n",
    "        else: \n",
    "            return True #add space\n",
    "    else:\n",
    "        return False #don't add space if no text in prev or if prev ends w/hyphen\n",
    "\n",
    "# Basic check to limit number of bullet points without period before them\n",
    "def is_capitalized_verb(paragraph):\n",
    "    if paragraph.strip()[0].isupper():\n",
    "        first_word = paragraph.split(' ')[0]\n",
    "        doc = nlp(paragraph)\n",
    "        pos = doc[0].pos_\n",
    "        if first_word == 'To' or pos=='VERB':\n",
    "            return True # either \"To\" or a captalized verb\n",
    "        else:\n",
    "            return False # not \"To\" or a capitalized verb\n",
    "    else:\n",
    "        return False # not capitalized\n",
    "\n",
    "# Combine words split by hyphen\n",
    "def hyphenation_correction(word):\n",
    "    word_split = word.split('-')\n",
    "    if len(word_split)==2 and len([word for word in word_split if word.strip()])==2:\n",
    "        # check if last letter of first word & first letter of last word is a letter\n",
    "        if word_split[0][-1].isalpha() and word_split[1][0].isalpha():\n",
    "            if is_split_word(word_split[0], word_split[1]):\n",
    "                return word_split[0] + word_split[1] #remove hyphen\n",
    "            else:\n",
    "                return word #leave hyphen\n",
    "        else:\n",
    "            return word #leave hyphen\n",
    "    else:\n",
    "        return word #leave hyphen\n",
    "\n",
    "def combine_words_split_by_hyphen(paragraph):\n",
    "    words = paragraph.split(' ')\n",
    "    new_para = ' '.join([word if ('-' not in word) else hyphenation_correction(word) for word in words])\n",
    "    return new_para\n",
    "\n",
    "# A manual inspection of the data revealed a number of headers/indices that repeated across multiple paragraphs\n",
    "# Remove these as they are not helpful for the analysis\n",
    "headers = [\"Leadership Message Our Perspective Products Planet Appendix GRI Index People\",\n",
    "           \"Introduction Our ESG Strategy Creating Shared Value Harm Reduction Environment Social Governance Performance and Assurance ESG Governance Environment\",\n",
    "           \"Corporate Governance Ethics and Compliance Respect for Human Rights Technology Employees Responsible Supply Chain Quality and Services Environment Community Engagement Management Message Approach to Sustainability Contents About the Sustainability Report\",\n",
    "           \"CEO Message Portfolio Data Appendix Our Company People Packaging Frameworks Water Operations Climate Agriculture\",\n",
    "           \"About This Report About Loblaw Message to Stakeholders Live Life Well Governance Environment Sourcing Community Targets\",\n",
    "           \"\\|\",\n",
    "           \"CEO Message Overview Team Members Customers Community Environment Supply Chain Governance SASB Index\",\n",
    "           \"Corporate Governance Ethics and Compliance Respect for Human Rights Technology Employees Responsible Supply Chain Quality and Services Environment Community Engagement Management Message Approach to Sustainability Contents About the Sustainability Report\",\n",
    "           \"Contents Editorial Policy Overview of Honda Message from the President and CEO Special Feature Sustainability Management Performance Report GRI Content Index Assurance Financial Data\",\n",
    "           \"Overview Editorial Policy Honda Philosophy Message from the President and CEO Sustainability Management GRI Content Index Assurance\",\n",
    "           \"Introduction Our ESG Strategy Creating Shared Value Harm Reduction Environment Social Governance Performance and Assurance ESG Governance Performance and Assurance\",\n",
    "           \"CEO Message Agriculture Portfolio Data Appendix Our Company People Packaging Frameworks Water Operations Climate\"]\n",
    "\n",
    "def regex_cleaning(paragraph):\n",
    "    # removing header number\n",
    "    paragraph = re.sub(r'^\\s?\\d+(.*)$', r'\\1', paragraph)\n",
    "    # removing trailing spaces\n",
    "    paragraph = paragraph.strip()\n",
    "    # words may be split between lines, ensure we link them back together\n",
    "    paragraph = re.sub('\\s?-\\s?', '-', paragraph)\n",
    "    # remove space prior to punctuation\n",
    "    paragraph = re.sub(r'\\s?([,:;\\.])', r'\\1', paragraph)\n",
    "    # replace numbers with '<NUM>'\n",
    "    # paragraph = re.sub('\\d+[,.]?\\d*[,.]?\\d*[%]?', '<NUM>', paragraph)\n",
    "    # remove mentions of URLs\n",
    "    paragraph = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', paragraph)\n",
    "    # remove multiple spaces\n",
    "    paragraph = re.sub('\\s+', ' ', paragraph)\n",
    "    return paragraph\n",
    "\n",
    "def remove_extra_headers(text):\n",
    "    text = re.sub('|'.join(headers), '', text)\n",
    "    return text\n",
    "\n",
    "# Checks whether a paragraph is made up mostly of capitalized words\n",
    "def mostly_cap(paragraph):\n",
    "    words_in_para = paragraph.split(' ')\n",
    "    words_in_para = [word for word in words_in_para if (word.strip() and not bool(re.search('\\d', word)))]\n",
    "    total_words = len(words_in_para)\n",
    "    upper_words = [word for word in words_in_para if word.strip()[0].isupper()]\n",
    "    if total_words > 0:\n",
    "        ratio = len(upper_words) / total_words\n",
    "        return ratio>.7 # will return true if most non-digit words start with a capital letter\n",
    "    else:\n",
    "        return True # paragraphs with no words (incl. paragraphs with only numbers / words with digits) will be deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2vB880CR1YRt",
   "metadata": {
    "id": "2vB880CR1YRt"
   },
   "source": [
    "### Main text extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FKPZtqI9Mz4v",
   "metadata": {
    "id": "FKPZtqI9Mz4v"
   },
   "outputs": [],
   "source": [
    "def extract_text(doc):\n",
    "    text_all = []\n",
    "    text_mcf = []\n",
    "    p_size = get_frequency_font_sizes(doc)\n",
    "    insufficient_blocking = None # NEED TO DEFINE\n",
    "    for page in doc:\n",
    "        # Get text in dict format, making sure to split ligatures into letters\n",
    "        blocks = page.get_text(\"dict\", sort=True,\n",
    "                              flags=~fitz.TEXT_PRESERVE_LIGATURES+fitz.TEXT_INHIBIT_SPACES)[\"blocks\"]\n",
    "        paragraphs_all, paragraphs_mcf = paragraphs_from_blocks(blocks, p_size)\n",
    "        text_all.extend(paragraphs_all)\n",
    "        text_mcf.extend(paragraphs_mcf)\n",
    "        \n",
    "    line_word_counts = [len(line.split(' ')) for line in text_mcf]\n",
    "\n",
    "    # Don't filter by font size if lines is empty\n",
    "    # or if there is no line with more than 5 words\n",
    "    if not line_word_counts or max(line_word_counts)<5:\n",
    "        cleaned_text = clean_paragraphs_from_blocks(text_all)\n",
    "    else:\n",
    "        cleaned_text = clean_paragraphs_from_blocks(text_mcf)\n",
    "    \n",
    "    return (text_all, text_mcf, cleaned_text)\n",
    "\n",
    "def paragraphs_from_blocks(blocks, p_size):\n",
    "    paragraphs_all = []\n",
    "    paragraphs_mcf = []\n",
    "    # ESG reports have a lot of multi-column pages so sort first horizontally \n",
    "    # (don't be too picky -> want to pick up indented lines in right order)\n",
    "    # and then sort vertically within those horizontal groups\n",
    "    for b in custom_sort(blocks): \n",
    "        paragraph_all_fonts = \"\"\n",
    "        paragraph_mcf = \"\"\n",
    "        for l in b[\"lines\"]:  # iterate through the text lines\n",
    "            for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                if s['text'].strip():  # removing lines w/just whitespaces:\n",
    "                    # add space to prev text if needed\n",
    "                    if need_to_add_space(paragraph_all_fonts, s['text']): \n",
    "                        paragraph_all_fonts += \" \"\n",
    "                    paragraph_all_fonts += s['text']\n",
    "                    if (p_size != None) and (round(s['size']) == p_size):\n",
    "                        # add space to prev text if needed\n",
    "                        if need_to_add_space(paragraph_mcf, s['text']):\n",
    "                            paragraph_mcf += \" \"\n",
    "                        paragraph_mcf += s['text']\n",
    "        paragraphs_all.append(paragraph_all_fonts)\n",
    "        paragraphs_mcf.append(paragraph_mcf)\n",
    "\n",
    "    return (paragraphs_all, paragraphs_mcf)\n",
    "\n",
    "def clean_paragraphs_from_blocks(paragraphs):\n",
    "    # Remove empty strings\n",
    "    paragraphs = [line for line in paragraphs if line] \n",
    "\n",
    "    # Remove non-ASCII characters (& strings with just non-ASCII characters)\n",
    "    paragraphs = [''.join(map(lambda x: x if x in string.printable and x not in string.whitespace else ' ', line)) for line in paragraphs]\n",
    "    paragraphs = [line.rstrip() for line in paragraphs if line.strip()]\n",
    "\n",
    "    # Remove number-only lines\n",
    "    paragraphs = [line for line in paragraphs if not line.isdigit()]\n",
    "    \n",
    "    # Remove headers (mark with '|' for easier paragraph splitting later):\n",
    "    #remove lines in all caps\n",
    "    paragraphs = [line if not line.isupper() else '|' for line in paragraphs]\n",
    "    #remove lines with fewer than 4 words where there is no period and the first letter is uppercase\n",
    "    paragraphs = [line if ('.' in line) or (len(line.strip().split(' '))>=4) or (not line[0].strip().isupper()) else '|' for line in paragraphs]\n",
    "    paragraphs = [line if not (('....' in line) or ('. . . .' in line)) else '|' for line in paragraphs]\n",
    "    #remove GRI/other index lines ('###-#:','G4-','GRI ','F ','P ','EC#', 'EN#','LA#','HR#','SO#', 'PR#','DMA-','FC#','principle #','DMA ')\n",
    "    # MAYBE ALSO ADD: #.# uppercase or #.## uppercase\n",
    "    paragraphs = [line if not bool(re.match(r'[0-9]{3}-[0-9]:|G4-|GRI \\d|A\\d|B\\d|F |P |EC\\s?[0-9]|FC\\s?[0-9]|EN\\s?[0-9]|LA\\s?[0-9]|HR\\s?[0-9]|SO\\s?[0-9]|PR\\s?[0-9]|DMA-|principle [0-9]|DMA |[0-9].[0-9] [A-Z]|[0-9].[0-9]{2} [A-Z]',line)) else '|' for line in paragraphs]\n",
    "\n",
    "    # Combine paragraphs if needed\n",
    "    cleaned_paragraphs =[]\n",
    "    prev = \"\"\n",
    "    for paragraph in paragraphs:\n",
    "        # paragraph will be appended to prev paragraph if:\n",
    "        # (1) its first char is lowercase and/or the previous line doesn't end in a period\n",
    "        # (2) paragraph and/or prev paragraph are not '|' (aka was a header)\n",
    "        if (paragraph.strip()[0].islower() or not prev.strip().endswith('.')) and not (paragraph=='|' or prev=='|'):\n",
    "            # check for words split across lines; combine if needed, otherwise add space & combine\n",
    "            if need_to_add_space(prev, paragraph):\n",
    "                # check if line is a bullet point (common when listing goals/risks, etc. in ESG reports\n",
    "                # and usually seen if line starts with space or capitalized \"To\" or some verb);\n",
    "                # if so, add a period to previous paragraph for better comprehension\n",
    "                if not prev.strip().endswith(('.',';',',',':')) and (paragraph.startswith(' ') or is_capitalized_verb(paragraph)):\n",
    "                    prev += '.'\n",
    "                \n",
    "                # add space\n",
    "                prev = prev + ' ' + paragraph\n",
    "            \n",
    "            else: # don't add space\n",
    "                prev = prev + paragraph\n",
    "        else:\n",
    "            # New paragraph\n",
    "            cleaned_paragraphs.append(prev)\n",
    "            prev = paragraph\n",
    "\n",
    "    # & don't forget left-over paragraph\n",
    "    cleaned_paragraphs.append(prev)\n",
    "\n",
    "    # Now we can remove the header placeholders\n",
    "    cleaned_paragraphs = [paragraph for paragraph in cleaned_paragraphs if not paragraph=='|']\n",
    "\n",
    "    # Regex-based cleaning\n",
    "    cleaned_paragraphs = [regex_cleaning(paragraph) for paragraph in cleaned_paragraphs]\n",
    "\n",
    "    # Get rid of empty paragraphs\n",
    "    cleaned_paragraphs = [paragraph for paragraph in cleaned_paragraphs if paragraph]\n",
    "\n",
    "    # Get rid of paragraphs that are mostly upper case\n",
    "    cleaned_paragraphs = [paragraph for paragraph in cleaned_paragraphs if not mostly_cap(paragraph)]\n",
    "\n",
    "    # Remove hyphens that are splitting words\n",
    "    cleaned_paragraphs = [combine_words_split_by_hyphen(paragraph) for paragraph in cleaned_paragraphs]\n",
    "\n",
    "    # There are certain headers that seem to recur; remove these\n",
    "    cleaned_paragraphs = [remove_extra_headers(header) for paragraph in cleaned paragraphs]\n",
    "\n",
    "    return cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b2be4f-1a0d-4ff7-9c4e-f56e3fe0f823",
   "metadata": {},
   "source": [
    "### Text to dict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3py9NRSlXdMe",
   "metadata": {
    "id": "3py9NRSlXdMe"
   },
   "outputs": [],
   "source": [
    "# Set keywords for checking whether supply chain related\n",
    "sc_keywords = ['supplier', 'supply chain', 'value chain', 'procurement', 'vendor', ' sourcing']\n",
    "\n",
    "def cleaned_text_to_dict(paragraphs):\n",
    "    paragraph_dict = {}\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        paragraph_dict[i] = {}\n",
    "        paragraph_dict[i]['Paragraph'] = paragraph\n",
    "\n",
    "        # Check for any substrings in the supply chain keywords list\n",
    "        if any(substring in paragraph_dict[i]['Paragraph'].lower() for substring in sc_keywords):\n",
    "            paragraph_dict[i]['Supply_Chain'] = 'Yes'\n",
    "        else:\n",
    "            paragraph_dict[i]['Supply_Chain'] = 'No'\n",
    "    return paragraph_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b88448-9bc4-4330-8f96-b36c2ec2ad5f",
   "metadata": {
    "id": "c6b88448-9bc4-4330-8f96-b36c2ec2ad5f"
   },
   "source": [
    "## Extract & clean text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e608d-755d-4b6d-8e1c-853c315ba876",
   "metadata": {},
   "source": [
    "This section extracts and cleans the PDF text and saves the text to .txt files which are organized in folders by company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec675d9c-aa7f-497f-8154-57cd4b8fa1ff",
   "metadata": {
    "id": "ec675d9c-aa7f-497f-8154-57cd4b8fa1ff"
   },
   "outputs": [],
   "source": [
    "# Make folders for all tickers\n",
    "for company in list(esg_urls_pd_m['Exchange-Ticker'].unique()):\n",
    "    path = f\"../Text files/{company}\"\n",
    "    if os.path.exists(path) == False:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8681e5-db8e-457e-a0df-3e6545bf953a",
   "metadata": {
    "id": "7e8681e5-db8e-457e-a0df-3e6545bf953a"
   },
   "outputs": [],
   "source": [
    "esg_urls_pd_m.set_index(\"filename\", inplace=True)\n",
    "to_download_list = esg_urls_pd_m[esg_urls_pd_m['to_download']==True].index.values.tolist()\n",
    "report_details_dict = esg_urls_pd_m[esg_urls_pd_m['URL'].notna()].to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bfe2c-fcda-46ba-9434-d30b4595adc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a4bfe2c-fcda-46ba-9434-d30b4595adc9",
    "outputId": "7a5739f9-9425-4774-d8f8-da5574189cf8"
   },
   "outputs": [],
   "source": [
    "# Access file and extract & save text without downloading full PDFs\n",
    "exceptions = {}\n",
    "\n",
    "for filename in tqdm(to_download_list):    \n",
    "    url = esg_urls_pd_m.loc[filename]['URL']\n",
    "    company = esg_urls_pd_m.loc[filename]['Exchange-Ticker']\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        f = io.BytesIO(r.content)\n",
    "        txt_path_all = f\"../Text files/{company}/{filename}.txt\"\n",
    "        txt_path_most_common_fontsize = f\"../Text files/{company}/{filename}-mcf.txt\"\n",
    "        txt_path_cleaned = f\"../Text files/{company}/{filename}-c.txt\"\n",
    "        with fitz.open(stream=f) as doc:\n",
    "            report_details_dict[filename]['Num_pages'] = len(doc)\n",
    "            text_all, text_most_common_fontsize, cleaned_text = extract_text(doc)\n",
    "            with open(txt_path_all, \"w\") as file:\n",
    "                for element in text_all:\n",
    "                    file.write(element + \"\\n\")\n",
    "            with open(txt_path_most_common_fontsize, \"w\") as file:\n",
    "                for element in text_most_common_fontsize:\n",
    "                    file.write(element + \"\\n\")\n",
    "            with open(txt_path_cleaned, \"w\") as file:\n",
    "                for element in cleaned_text:\n",
    "                    file.write(element + \"\\n\")\n",
    "    except Exception as e:\n",
    "        exceptions[filename] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jVfHbecUuPdM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVfHbecUuPdM",
    "outputId": "6108bf67-4ebb-41bb-e759-d8055379b7d5"
   },
   "outputs": [],
   "source": [
    "# See how many PDFs weren't successfully converted with above code\n",
    "print(len(exceptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XWfytm0Gbp34",
   "metadata": {
    "id": "XWfytm0Gbp34"
   },
   "outputs": [],
   "source": [
    "# Transform downloaded PDFs into text files\n",
    "\n",
    "for folder in os.listdir(f\"../Downloaded_reports/\"):\n",
    "    if not folder.startswith(\".\"):\n",
    "        for filename in os.listdir(f\"../Downloaded_reports/{folder}\"):\n",
    "            if filename.endswith(('.pdf','.PDF')):\n",
    "                company = esg_urls_pd_m.loc[filename[:-4]]['Exchange-Ticker']\n",
    "                pdf_path = f\"../Initially_downloaded_reports/{folder}/{filename}\"\n",
    "                txt_path_all = f\"../Text files/{company}/{filename[:-4]}.txt\"\n",
    "                txt_path_most_common_fontsize = f\"../Text files/{company}/{filename[:-4]}-mcf.txt\"\n",
    "                txt_path_cleaned = f\"../Text files/{company}/{filename[:-4]}-c.txt\"\n",
    "                with fitz.open(pdf_path) as doc:\n",
    "                    report_details_dict[filename[:-4]]['Num_pages'] = len(doc)\n",
    "                    text_all, text_most_common_fontsize, cleaned_text = extract_text(doc)\n",
    "                    with open(txt_path_all, \"w\") as file:\n",
    "                        for element in text_all:\n",
    "                            file.write(element + \"\\n\")\n",
    "                    with open(txt_path_most_common_fontsize, \"w\") as file:\n",
    "                        for element in text_most_common_fontsize:\n",
    "                            file.write(element + \"\\n\")\n",
    "                    with open(txt_path_cleaned, \"w\") as file:\n",
    "                        for element in cleaned_text:\n",
    "                            file.write(element + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d67179-394d-4524-aead-028e12d3d0a8",
   "metadata": {
    "id": "68d67179-394d-4524-aead-028e12d3d0a8"
   },
   "outputs": [],
   "source": [
    "# Create list of remaining text files\n",
    "remaining_list = []\n",
    "for filename in list(esg_urls_pd_m[esg_urls_pd_m['URL'].notna()].index.values):\n",
    "    company = esg_urls_pd_m.loc[filename]['Exchange-Ticker']\n",
    "    txt_path = f\"../Text files/{company}/{filename}.txt\"\n",
    "    if os.path.exists(txt_path) == False:\n",
    "        remaining_list.append(filename)\n",
    "        \n",
    "# Make sure there aren't any missing text files\n",
    "if len(remaining_list) > 0:\n",
    "    print(\"The following files are missing:\")\n",
    "    for filename in remaining_list:\n",
    "        print(filename)\n",
    "else:\n",
    "    print(\"No missing files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6QwcQbGjXMld",
   "metadata": {
    "id": "6QwcQbGjXMld"
   },
   "source": [
    "## Classify as SC and extract sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9sD8xKrW6I9u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sD8xKrW6I9u",
    "outputId": "6c17175a-ea98-4491-9ab5-30d9838c7367"
   },
   "outputs": [],
   "source": [
    "# Save additional report metadata to report details dict (report_details_dict)\n",
    "# Save paragraphs to a dataframe (paragraph_df)\n",
    "no_file_available = []\n",
    "paragraph_df = pd.DataFrame(columns=['Paragraph_Order', 'Filename', 'Paragraph', 'Supply_Chain'])\n",
    "for filename in tqdm(list(esg_urls_pd_m[esg_urls_pd_m['URL'].notna()].index.values)):\n",
    "    company = esg_urls_pd_m.loc[filename]['Exchange-Ticker']\n",
    "    path_cleaned = f\"../Text files/{company}/{filename}-c.txt\"\n",
    "    if os.path.exists(path_cleaned):\n",
    "        with open(path_cleaned) as f:\n",
    "            paragraphs = f.readlines()\n",
    "            paragraph_dict = cleaned_text_to_dict(paragraphs)\n",
    "        report_details_dict[filename]['Num_paras'] = len(paragraph_dict)\n",
    "        report_details_dict[filename]['Num_SC_paras'] = len({k:v for k,v in paragraph_dict.items() if v['Supply_Chain']=='Yes'})\n",
    "        \n",
    "        # Add to DF\n",
    "        temp_df = pd.DataFrame.from_dict(paragraph_dict, orient='index')\n",
    "        temp_df['Filename'] = filename\n",
    "        temp_df.reset_index(inplace=True)\n",
    "        temp_df.rename(columns={'index':'Paragraph_Order'}, inplace=True)\n",
    "        paragraph_df = pd.concat([paragraph_df, temp_df])\n",
    "    else:\n",
    "        no_file_available.append(filename)\n",
    "\n",
    "# Reset index to create Paragraph ID\n",
    "paragraph_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Create dataframes for report details and paragraphs w/o sents\n",
    "report_details_df = pd.DataFrame.from_dict(report_details_dict, orient='index')\n",
    "paragraph_df = paragraph_df[['Paragraph_Order', 'Filename', 'Paragraph', 'Supply_Chain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ve2_bvkKMJD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ve2_bvkKMJD",
    "outputId": "9eda2c5b-2204-4335-8c72-efe777956566"
   },
   "outputs": [],
   "source": [
    "print(len(paragraph_df)) #363,950 paragraphs\n",
    "print(len(paragraph_df[paragraph_df['Supply_Chain']=='Yes'])) #52,495 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phGwkCMudE4D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "phGwkCMudE4D",
    "outputId": "6dba8c34-d354-4945-ee75-5c2cb9f525f7"
   },
   "outputs": [],
   "source": [
    "paragraph_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5S-QT5yyiI48",
   "metadata": {
    "id": "5S-QT5yyiI48"
   },
   "source": [
    "## Save data to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfvk7qhKiIVM",
   "metadata": {
    "id": "cfvk7qhKiIVM"
   },
   "outputs": [],
   "source": [
    "report_details_df.to_pickle('../Data/report_details_df.pkl')\n",
    "paragraph_df.to_pickle('../Data/paragraph_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "Get report text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
