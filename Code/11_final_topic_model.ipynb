{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d8ecb1-4e58-4d8f-8257-f1d3205fdf53",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa4bbd-2fc3-46e4-94cc-7b4b94ec940f",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e04581-f01b-4b18-a47f-74508df3406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#For plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd15f34-f7e6-4380-b536-92659bbdd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df = pd.read_pickle('../Data/paragraph_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ff6eb-aeda-4ee3-ae3f-6cd52c64d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_only_para_df = paragraph_df[paragraph_df['Supply_Chain']=='Yes']\n",
    "print(len(sc_only_para_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f78de-b954-4734-80ef-1fc2303957df",
   "metadata": {},
   "source": [
    "## Functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be153ab5-66ec-4f8b-a652-8cecea444d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "stop_words = stopwords.words('english')\n",
    "additional_stop_words = ['report', 'annualreport','esg','sustainability',\n",
    "                         'sustainable','also','business','group','company','year'] \n",
    "sc_keywords = ['supplier', 'suppliers', 'supply', 'chain', 'chains', \n",
    "               'procurement', 'vendor', 'vendors', 'sourcing']\n",
    "\n",
    "stop_words += additional_stop_words\n",
    "stop_words += sc_keywords\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1e5f3-3ff1-4089-aabd-c6ff11a4ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to nouns, adjectives, and adverbs (???-check whether I want to keep this)\n",
    "def paragraphs_to_words(paragraphs):\n",
    "    for paragraph in paragraphs:\n",
    "        # note: deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(paragraph), deacc=True))\n",
    "\n",
    "def pos_and_location_filter(text):\n",
    "    doc = nlp(text)\n",
    "    included_tags = [\"NOUN\",\"ADJ\",\"ADV\"]\n",
    "    ents = [e.text for e in doc.ents if e.label_ in ['GPE','LOC','ORG']]\n",
    "    filtered_text = [item.text for item in doc if (not item.text in ents) and (item.pos_ in included_tags)]\n",
    "    return filtered_text\n",
    "        \n",
    "def text_preprocessing(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('[,\\.!?]', '', text) # remove punctuation\n",
    "    text = ''.join(i for i in text if not i.isdigit()) # remove numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # removes non-letter characters\n",
    "    \n",
    "    text = text.lower()\n",
    "    text_list = pos_and_location_filter(text)\n",
    "    text_list = [w for w in text_list if w not in stop_words]\n",
    "    text_list = [lemmatizer.lemmatize(word) for word in text_list]\n",
    "    return ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd2028-27c7-427c-a00e-161171c2cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_texts_corpus(list_of_paragraphs):\n",
    "    texts = list(paragraphs_to_words(list_of_paragraphs))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Trigram texts\n",
    "    texts_w_trigrams = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    id2word = corpora.Dictionary(texts_w_trigrams)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts_w_trigrams]\n",
    "    \n",
    "    return bigram_mod, trigram_mod, texts_w_trigrams, id2word, corpus, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b9948-6c25-46be-8cb5-cf299725837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the topics per doc\n",
    "def get_predicted_topics(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70224e84-3c88-43e6-afe0-bff13b49f79d",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad85d53-37ec-4ed8-9654-c86a678e0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_only_para_df['Processed_Text'] = sc_only_para_df['Paragraph'].map(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dceb6c-a150-41c9-97ac-691f4d05bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All docs\n",
    "total_list_of_paragraphs = sc_only_para_df['Processed_Text'].values.tolist()\n",
    "total_bigram_mod, total_trigram_mod, total_texts_w_trigrams, total_id2word, total_corpus, total_texts = create_dict_texts_corpus(total_list_of_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394776da-9292-4aac-a217-bc1731135093",
   "metadata": {},
   "source": [
    "## Build topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c35ede-39f2-402e-85aa-1891b088fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model20 = gensim.models.ldamodel.LdaModel(corpus=total_corpus,\n",
    "                                            id2word=total_id2word,\n",
    "                                            num_topics=20, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=1000,\n",
    "                                            passes=10)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(model20, total_corpus, total_id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d4caa-a446-4947-b4fb-ebf2c25bdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_topic = model20.show_topics(num_topics=20, num_words=15)\n",
    "\n",
    "for i, topic_words in words_per_topic:\n",
    "    formatted_topic_words = topic_words.split(\" + \")\n",
    "    formatted_topic_words = [''.join(i for i in word if i.isalpha()) for word in formatted_topic_words]\n",
    "    print(f\"Topic {i}: {formatted_topic_words}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33f93b-d779-4866-aade-7957ffd6144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = get_predicted_topics(ldamodel=model20, \n",
    "                                                  corpus=total_corpus, texts=total_texts_w_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e76293-c521-42f7-bb6c-46cf3a5ac0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0:'Management systems',1:'Deforestation',2:'Human rights',3:'Employee health & safety',\n",
    "               4:'Resource usage',\n",
    "               5:'Certifications & training',6:'Collaborations & partnerships',7:'Plans and progress',\n",
    "               8:'Governance & stakeholders',9:'Policies',\n",
    "               10:'Product quality',11:'Diversity & inclusion',12:'Junk',13:'Agriculture',\n",
    "               14:'Risk assessments',\n",
    "               15:'Chemicals',16:'Transportation & logistics',\n",
    "               17:'Society',18:'Store operations',19:'Materials & packaging'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c69247-c9cd-474f-981a-bbbd24c2a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords['Dominant_Topic_Named'] = df_topic_sents_keywords['Dominant_Topic'].map(lambda x: topic_names[round(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea55ba3-9892-46e2-91f0-6498f3d643b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sc_df = pd.concat([sc_only_para_df.reset_index(),df_topic_sents_keywords], axis=1)\n",
    "merged_sc_df = merged_sc_df.merge(report_details_df[['Company Name_x','Year','Main industry','Filename']],on='Filename')\n",
    "merged_sc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c87c6-ddb8-41d0-9ece-024c05617a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_group_mapping = {'Management systems':'Actions','Deforestation':'Environment',\n",
    "                       'Human rights':'Social','Employee health & safety':'Social',\n",
    "                       'Resource usage':'Environment',\n",
    "                       'Certifications & training':'Actions',\n",
    "                       'Collaborations & partnerships':'Actions',\n",
    "                       'Plans and progress':'Actions',\n",
    "                       'Governance & stakeholders':'Actions','Policies':'Actions',\n",
    "                       'Product quality':'Social',\n",
    "                       'Diversity & inclusion':'Social','Junk':None,'Agriculture':'Environment',\n",
    "                       'Risk assessments':'Actions','Chemicals':'Environment',\n",
    "                       'Transportation & logistics':'Environment',\n",
    "                       'Society':'Social','Store operations':'Social',\n",
    "                       'Materials & packaging':'Environment'}\n",
    "\n",
    "merged_sc_df['Groups'] = merged_sc_df[\"Dominant_Topic_Named\"].map(topic_group_mapping)\n",
    "merged_sc_df['Groups'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358247be-200c-484c-9e34-b2383011f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sc_df.to_pickle('../Data/paragraphs_w_topics.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
